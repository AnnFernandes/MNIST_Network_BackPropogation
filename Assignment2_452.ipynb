{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_452.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjra1wEbz1qm",
        "colab_type": "text"
      },
      "source": [
        "Assignment 2 - MNIST Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4obLwMDiQwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sigmoid function for activation\n",
        "def sigmoid(z):\n",
        "    s = 1 / (1 + np.exp(-z))\n",
        "    return s\n",
        "\n",
        "#function to one hot encode the array given - output\n",
        "def one_hot_encoding(output, num_labels=10):\n",
        "  one_h = np.zeros((output.shape[0], num_labels))\n",
        "  for x, y in enumerate(output):\n",
        "    one_h[x,y] = 1.0\n",
        "  return one_h\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pRshew6Dd03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing the dataset\n",
        "from sklearn.datasets import load_digits, fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm \n",
        "\n",
        "\n",
        "x,y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "#training - 60000x784 input, 60000x1 output\n",
        "#testing - 10000x784 input, 10000x1 output\n",
        "#splitting the testing and training data\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x,y, train_size=60000, test_size=10000, shuffle=False)\n",
        "\n",
        "xtrain = xtrain.astype(np.float)\n",
        "xtest = xtest.astype(np.float)\n",
        "\n",
        "#normalizing the data so don't compute with high numbers\n",
        "xtrain = xtrain/255\n",
        "xtest = xtest/255\n",
        "\n",
        "#from string to integer for output\n",
        "ytrain = ytrain.astype(np.int)\n",
        "ytest = ytest.astype(np.int)\n",
        "    \n",
        "#transform output into one hot encoding for both test and training sets\n",
        "ytrain = one_hot_encoding(ytrain)\n",
        "yt = one_hot_encoding(ytest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dp_KZc9Qd1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron(object):\n",
        "\n",
        "    #define the attirubutes for the function\n",
        "    def __init__(self, num_hidden, epoch=22, c=0.01):\n",
        "        self.epoch = epoch #iteration\n",
        "        self.c = c #learning rate\n",
        "        self.input_size = 784 #inputs\n",
        "        self.output_size = 10 #output\n",
        "        self.hidden = num_hidden #neurons in the hidden layer\n",
        "        \n",
        "        print('Initialization with random weight')\n",
        "        print('-----')\n",
        "        self.weight1 = np.random.normal(0, 0.2, [self.input_size, self.hidden]) #weights in the first layer between input and hidden\n",
        "        self.bias1 = np.zeros((1, self.hidden)) # bias for weight 1\n",
        "        self.weight2 = np.random.normal(0, 0.2, [self.hidden, self.output_size]) #weights in the second layer between hidden and output\n",
        "        self.bias2 = np.zeros((1, self.output_size))  # bias for weight 2\n",
        "      \n",
        "    #Used to train the network - takes the training input and output, and batch value\n",
        "    def train(self, training_inputs, training_outputs, batch):\n",
        "      \n",
        "        print('Network training with a '+str(batch)+' batch size')\n",
        "        print('-----')\n",
        "        \n",
        "        #momentum value\n",
        "        beta = 0.4\n",
        "        #intializing previous gradients \n",
        "        prev_grad_w2 = 0\n",
        "        prev_grad_w1 = 0\n",
        "        \n",
        "        #for the number of epochs specified\n",
        "        pbar = tqdm(range(self.epoch))\n",
        "        for k in pbar:\n",
        "          it = 0\n",
        "          avg_error = 0\n",
        "          while it < training_inputs.shape[0]:\n",
        "                \n",
        "                #take the batch size of inputs and outputs\n",
        "                tx = training_inputs[it:it+batch]\n",
        "                ty = training_outputs[it:it+batch]\n",
        "                \n",
        "                # forward pass\n",
        "                z1 = np.dot(tx, self.weight1) + self.bias1\n",
        "                a1 = sigmoid(z1)\n",
        "                z2 = np.dot(a1, self.weight2) + self.bias2\n",
        "                y = sigmoid(z2)\n",
        "                \n",
        "                #backpropogation\n",
        "                dy2 = y*(1-y) #derivative of final output\n",
        "                dy1 = a1*(1-a1) #derivative of hidden layer output\n",
        "                output_error = np.subtract(ty,y) #error in output layer\n",
        "                avg_error += np.mean(output_error)\n",
        "                output_delta = output_error * dy2 #element wise mulitplication\n",
        "                hidden_error = np.dot(output_delta, self.weight2.T)\n",
        "                hidden_layer_delta = hidden_error * dy1 #element wise multiplication\n",
        "                \n",
        "                #momentum\n",
        "                momentum_factor_w1 = beta * prev_grad_w1\n",
        "                momentum_factor_w2 = beta * prev_grad_w2\n",
        "                \n",
        "                #backpropogation calculation of each weight and bias\n",
        "                dW2 = np.dot(a1.T, output_delta) # forward * backward\n",
        "                db2 = np.sum(output_delta, axis = 0, keepdims = True)\n",
        "                dW1 = np.dot(tx.T, hidden_layer_delta)\n",
        "                db1 = np.sum(hidden_layer_delta, axis = 0, keepdims = True)\n",
        "                \n",
        "                \n",
        "                #update weights and bias\n",
        "                self.weight2 += (1./batch * self.c * dW2) + momentum_factor_w2\n",
        "                self.bias2 +=  1./batch * self.c * db2\n",
        "                self.weight1 +=  (1./batch * self.c * dW1) + momentum_factor_w1 \n",
        "                self.bias1 +=  1./batch * self.c * db1\n",
        "                \n",
        "                #save the graident to use in next batch iteration\n",
        "                prev_grad_w1 = 1./batch * self.c * dW1\n",
        "                prev_grad_w2 = 1./batch * self.c * dW2\n",
        "                \n",
        "                #input(\"Wait\")\n",
        "                \n",
        "                #increment to access next set of inputs\n",
        "                it = it + batch\n",
        "          avg_error /= float(training_inputs.shape[0])\n",
        "          pbar.set_description(\"Error: %.4f\" %(avg_error))\n",
        "        \n",
        "\n",
        "  # Used to test the neural network - accepts the x or input values of the test data\n",
        "    def test(self, test_input, test_output):\n",
        "      #forward feed\n",
        "      z1 = np.dot(test_input, self.weight1) + self.bias1\n",
        "      a1 = sigmoid(z1)\n",
        "      z2 = np.dot(a1, self.weight2) + self.bias2\n",
        "      y = sigmoid(z2)\n",
        "      \n",
        "      a = np.zeros((10000, 1))\n",
        "     \n",
        "      # To see how accurate the neural network is compared to the correct values\n",
        "      acc = 0.0\n",
        "      for i in range(10000):\n",
        "        if np.argmax(y[i]) == np.argmax(test_output[i]):\n",
        "          acc += 1\n",
        "          a[i] = np.argmax(y[i])\n",
        "      print(acc / 10000 * 100, \"%\")\n",
        "      \n",
        "      a = a.astype(np.int)\n",
        "      \n",
        "      return a\n",
        "\n",
        "    def output_data_txt(self, true_output, predictions):\n",
        "      #outputs predicated and weights into a file on the drive - need to give it access\n",
        "      from google.colab import drive\n",
        "      drive.mount('/content/gdrive', force_remount=True)\n",
        "      \n",
        "      #print final weights into a text file\n",
        "      with open('/content/gdrive/My Drive/Output/finalweight.txt', 'w') as f:\n",
        "        np.savetxt(f, self.weight1, fmt='%.5f', delimiter=',', header='Weight1')\n",
        "        np.savetxt(f, self.bias1, fmt='%.5f', delimiter=',', header='Bias1')\n",
        "        np.savetxt(f, self.weight2, fmt='%.5f', delimiter=',', header='Weight2')\n",
        "        np.savetxt(f, self.bias2, fmt='%.5f', delimiter=',', header='Bias2')\n",
        "      \n",
        "      #combine the predictions and true value into 1 matrix (so can see side beside)\n",
        "      true_output = np.reshape(true_output, (10000, 1))\n",
        "      final =  np.concatenate( [ predictions, true_output ] , axis = 1)\n",
        "      #print predications an true value into an output file on google drive\n",
        "      with open('/content/gdrive/My Drive/Output/outputs.txt', 'w') as d:\n",
        "        np.savetxt(d, final, fmt='%s',  header = \"Predictions vs True Value\")\n",
        "        \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JD6LU0H82LV",
        "colab_type": "code",
        "outputId": "7cb0b12a-9334-4560-b4f3-fceec720f776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Create nueral network (intialize)\n",
        "per = Perceptron(64)\n",
        "#Train network\n",
        "per.train(xtrain,ytrain, 32)\n",
        "#Test network\n",
        "prediction = per.test(xtest, yt)\n",
        "#Output final weights and predicated values into a text file\n",
        "per.output_data_txt(ytest, prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initialization with random weight\n",
            "-----\n",
            "Network training with a 32 batch size\n",
            "-----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Error: -0.0008:   0%|          | 0/22 [00:02<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0008:   5%|▍         | 1/22 [00:02<00:48,  2.29s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0007:   5%|▍         | 1/22 [00:04<00:48,  2.29s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0007:   9%|▉         | 2/22 [00:04<00:46,  2.31s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0010:   9%|▉         | 2/22 [00:06<00:46,  2.31s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0010:  14%|█▎        | 3/22 [00:06<00:43,  2.29s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0011:  14%|█▎        | 3/22 [00:09<00:43,  2.29s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0011:  18%|█▊        | 4/22 [00:09<00:41,  2.28s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0011:  18%|█▊        | 4/22 [00:11<00:41,  2.28s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0011:  23%|██▎       | 5/22 [00:11<00:38,  2.28s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0010:  23%|██▎       | 5/22 [00:13<00:38,  2.28s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0010:  27%|██▋       | 6/22 [00:13<00:36,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0010:  27%|██▋       | 6/22 [00:15<00:36,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0010:  32%|███▏      | 7/22 [00:16<00:34,  2.29s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0009:  32%|███▏      | 7/22 [00:18<00:34,  2.29s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0009:  36%|███▋      | 8/22 [00:18<00:31,  2.28s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0008:  36%|███▋      | 8/22 [00:20<00:31,  2.28s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0008:  41%|████      | 9/22 [00:20<00:29,  2.28s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0008:  41%|████      | 9/22 [00:22<00:29,  2.28s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0008:  45%|████▌     | 10/22 [00:22<00:27,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0007:  45%|████▌     | 10/22 [00:25<00:27,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0007:  50%|█████     | 11/22 [00:25<00:24,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0007:  50%|█████     | 11/22 [00:27<00:24,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0007:  55%|█████▍    | 12/22 [00:27<00:22,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0006:  55%|█████▍    | 12/22 [00:29<00:22,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0006:  59%|█████▉    | 13/22 [00:29<00:20,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0006:  59%|█████▉    | 13/22 [00:31<00:20,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0006:  64%|██████▎   | 14/22 [00:31<00:18,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0006:  64%|██████▎   | 14/22 [00:34<00:18,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0006:  68%|██████▊   | 15/22 [00:34<00:15,  2.26s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0005:  68%|██████▊   | 15/22 [00:36<00:15,  2.26s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0005:  73%|███████▎  | 16/22 [00:36<00:13,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0005:  73%|███████▎  | 16/22 [00:38<00:13,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0005:  77%|███████▋  | 17/22 [00:38<00:11,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0005:  77%|███████▋  | 17/22 [00:40<00:11,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0005:  82%|████████▏ | 18/22 [00:40<00:09,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0005:  82%|████████▏ | 18/22 [00:43<00:09,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0005:  86%|████████▋ | 19/22 [00:43<00:06,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0004:  86%|████████▋ | 19/22 [00:45<00:06,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0004:  91%|█████████ | 20/22 [00:45<00:04,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0004:  91%|█████████ | 20/22 [00:47<00:04,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0004:  95%|█████████▌| 21/22 [00:47<00:02,  2.26s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0004:  95%|█████████▌| 21/22 [00:50<00:02,  2.26s/it]\u001b[A\u001b[A\n",
            "\n",
            "Error: -0.0004: 100%|██████████| 22/22 [00:50<00:00,  2.28s/it]\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "89.37 %\n",
            "Mounted at /content/gdrive\n",
            "(10000, 1)\n",
            "(10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}